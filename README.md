## Summary of the Project
For this project, I will be reviewing “Consistent high-dimensional Bayesian variable selection via penalized credible regions” (Bondell and Reich, 2012) and using “Priors for Bayesian shrinkage and high-dimension model selection” (Shin, 2017) as a supplementary source. This paper studies a new Bayesian variable selection method in linear regression models called penalized credible regions approach. The second paper provides some instructions on the dataset that I will be using and lists some methods of performance measurement. I will be extensively summarizing the selected paper into four parts: introduction, penalized credible regions, asymptotic theory, and simulations. Next, I will be implementing two types of the proposed approach, the joint sets and the marginal sets approach, on Boston housing dataset in varying settings and compare the results with those of the traditional Lasso.

## Introduction of the Project
In the second half of the semester, we spent a great amount of time on Bayesian linear regression and variable selection, which I am very interested in. With the increasing dimensions of modern data and necessity to reduce dimensions, variable selection is a topic gaining popularity. Statisticians have designed a variety of methods for variable selection, including both frequentist and Bayesian approaches. Some frequentist methods include forward selection, backward elimination, least absolute shrinkage and selection operator, fused Lasso, adaptive Lasso and so on. On the other hand, the Bayesian statistics most frequently approaches the variable selection problems with a probability-based MCMC sampling call Stochastic Search Variable Selection (SSVS), which we focused a lot in class and assignments. There are many papers discussing the pros and cons of each method with respect to accuracy, computation time, whether predictors are correlated or not, dimension relative to sample size and so on. Finding the most suitable variable selection method for a specific dataset is always a debatable topic. The paper I selected is the same. This paper suggests that the penalized credible regions approach is a more efficient and more accurate method than other commonly used Bayesian and frequentist approach, and it exhibits selection consistency in high dimensional settings. Therefore, I choose to review this paper to see how the proposed approach differs from other else. The Boston housing dataset is then selected because it is frequently used to study variable selection.

## My Work and Extensions
In this part, I applied the penalized credible regions approach on Boston housing dataset in a variety of settings, compared the outcomes with the frequentist Lasso approach, and concluded a list of variables selected using each method. The Boston housing dataset has a sample size of 506 and a dimension of 14. Among them, there are 13 predictors (crim, zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, and lstat) and 1 response variable (medv), which is the median value of owner-occupied homes in $1,000s. Because there are only 13 predictors, we need to generate spurious irrelevant variables for variable selection (Shin, 2017). Hence, I decided to generate noise variables ∈{20, 400}to bring the dimension of the dataset to {33,413}, representing a low dimension and a high dimension respectively. Unlike the simulations in the paper, I only use this single one dataset instead of simulating multiple datasets. Furthermore, I am curious about how independence and correlations among covariates influence the accuracy of each method, so I sampled both independent noise variables from a standard normal distribution and correlated noise variables from N(0,1) with AR(1) and lag-1 correlation of ρ = 0.5. To measure performance, I used the ROC curve mentioned in the paper. Also, I incorporated two new measurements, MS-O and MS-N, which count the average number of selected original variables and selected noise variables respectively of the first 20 ordered sets from the complete solution path.

For the credible sets approach, I utilized an online package called ”BayesPen” and input data, hyperparameters of priors, number of iterations equaling 5,000, and number of burn-in equaling 500. The residuals are assumed to be identically and independently distri- bution as N(0,τ) with τ ∼Gamma(0.01,0.01). The regression coefficients also has a N(0,τ) distribution with τ ∼Gamma(0.01,0.01). The function “BayesPen.lm” automatically generated ordered sets, and the outputs offered a complete solution path of adding variables (from zero to all variables) for both joint and marginal approaches. For the Lasso regression, I simply used the “cv.glmnet” function with varying tuning parameter λ to create ordered sets.

Subsequently, I plotted the ROC curves of the complete solution paths through computing FPR and TPR. However, when calculating MS-O and MS-N, I used only the first 20 ordered sets from the complete paths. The first 20 ordered sets contain the variables that were added to the model the earliest. Ideally, the first 20 ordered sets should contain the 13 true variables. Below are the ROC plots under each setting.

![WeChat364f6130765dcc5255c8048bbfdb6951](https://github.com/user-attachments/assets/38bda528-c258-4c76-bdc3-677d05cd7135)

