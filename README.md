## Summary of the Project
For this project, I will be reviewing “Consistent high-dimensional Bayesian variable selection via penalized credible regions” (Bondell and Reich, 2012) and using “Priors for Bayesian shrinkage and high-dimension model selection” (Shin, 2017) as a supplementary source. This paper studies a new Bayesian variable selection method in linear regression models called penalized credible regions approach. The second paper provides some instructions on the dataset that I will be using and lists some methods of performance measurement. I will be extensively summarizing the selected paper into four parts: introduction, penalized credible regions, asymptotic theory, and simulations. Next, I will be implementing two types of the proposed approach, the joint sets and the marginal sets approach, on Boston housing dataset in varying settings and compare the results with those of the traditional Lasso.

## Introduction of the Project
In the second half of the semester, we spent a great amount of time on Bayesian linear regression and variable selection, which I am very interested in. With the increasing dimensions of modern data and necessity to reduce dimensions, variable selection is a topic gaining popularity. Statisticians have designed a variety of methods for variable selection, including both frequentist and Bayesian approaches. Some frequentist methods include forward selection, backward elimination, least absolute shrinkage and selection operator, fused Lasso, adaptive Lasso and so on. On the other hand, the Bayesian statistics most frequently approaches the variable selection problems with a probability-based MCMC sampling call Stochastic Search Variable Selection (SSVS), which we focused a lot in class and assignments. There are many papers discussing the pros and cons of each method with respect to accuracy, computation time, whether predictors are correlated or not, dimension relative to sample size and so on. Finding the most suitable variable selection method for a specific dataset is always a debatable topic. The paper I selected is the same. This paper suggests that the penalized credible regions approach is a more efficient and more accurate method than other commonly used Bayesian and frequentist approach, and it exhibits selection consistency in high dimensional settings. Therefore, I choose to review this paper to see how the proposed approach differs from other else. The Boston housing dataset is then selected because it is frequently used to study variable selection.

## My Work and Extensions
In this part, I applied the penalized credible regions approach on Boston housing dataset in a variety of settings, compared the outcomes with the frequentist Lasso approach, and concluded a list of variables selected using each method. The Boston housing dataset has a sample size of 506 and a dimension of 14. Among them, there are 13 predictors (crim, zn, indus, chas, nox, rm, age, dis, rad, tax, ptratio, black, and lstat) and 1 response variable (medv), which is the median value of owner-occupied homes in $1,000s. Because there are only 13 predictors, we need to generate spurious irrelevant variables for variable selection (Shin, 2017). Hence, I decided to generate noise variables ∈{20, 400}to bring the dimension of the dataset to {33, 413}, representing a low dimension and a high dimension respectively. Unlike the simulations in the paper, I only use this single one dataset instead of simulating multiple datasets. Furthermore, I am curious about how independence and correlations among covariates influence the accuracy of each method, so I sampled both independent noise variables from a standard normal distribution and correlated noise variables from N(0, 1) with AR(1) and lag-1 correlation of ρ = 0.5. To measure performance, I used the ROC curve mentioned in the paper. Also, I incorporated two new measurements, MS-O and MS-N, which count the average number of selected original variables and selected noise variables respectively of the first 20 ordered sets from the complete solution path.

For the credible sets approach, I utilized an online package called ”BayesPen” and input data, hyperparameters of priors, number of iterations equaling 5,000, and number of burn-in equaling 500. The residuals are assumed to be identically and independently distribution as N(0, τ) with τ ∼Gamma(0.01, 0.01). The regression coefficients also has a N(0, τ) distribution with τ ∼Gamma(0.01, 0.01). The function “BayesPen.lm” automatically generated ordered sets, and the outputs offered a complete solution path of adding variables (from zero to all variables) for both joint and marginal approaches. For the Lasso regression, I simply used the “cv.glmnet” function with varying tuning parameter λ to create ordered sets.

Subsequently, I plotted the ROC curves of the complete solution paths through computing FPR and TPR. However, when calculating MS-O and MS-N, I used only the first 20 ordered sets from the complete paths. The first 20 ordered sets contain the variables that were added to the model the earliest. Ideally, the first 20 ordered sets should contain the 13 true variables. Below are the ROC plots under each setting.

![4](https://github.com/user-attachments/assets/db2586b7-7ac8-4dfc-8e7b-c79390bae6d9)

![5](https://github.com/user-attachments/assets/482651dc-dc3a-46e3-8346-913fb071291e)

![6](https://github.com/user-attachments/assets/4e1a406c-63d2-485f-a411-0748908e2b11)

![7](https://github.com/user-attachments/assets/e169a7f6-2904-489c-a0cd-8411252e7c81)

From the above plots, we can tell in most cases, the joint and marginal credible regions approaches have similar ROC curves and area under the curve (AUC). The joint approach performs slightly better than the marginal approach when variables are not correlated. Increasing dimensions and correlation does not reduce the AUC for these two methods. On the other hand, the Lasso approach underperforms both credible sets approaches most of the time, especially given a higher-dimensional setting, but it is surprising to see that in a low dimensional setting, when the correlation increases from 0 to 0.5, the Lasso regression does not perform worse. This finding somehow contradicts the statement in the paper that Lasso suffers greatly from correlations among variables. Hence, this contradiction is worth further studying. After that, I created a table for MS-O and MS-N.

![t1](https://github.com/user-attachments/assets/9764a7c2-3bc4-47ce-9a4e-1843ce812df8)

Table 1 compares the MS-O and the MS-N for the first 20 ordered sets under each setting. The marginal method keeps selecting the most original predictors in all cases, followed by the joint approach, but the marginal approach also has a higher MS-N than the joint one. Moreover, I notice that when pis large, the MS-O of credible sets methods improves and the MS-N drops as we increase the correlation. This suggests that the proposed approach yields an even more accurate outcome using correlated predictors. In a low dimensional setting, the Lasso regression always selects the fewest original variables and the most noise variables. With a higher dimension, the average numbers of selected original variables of Lasso are down to 1.55 and 1.6, which are very low and unreasonable. Meanwhile, the MS-N is down to 0 for both independent and correlated variables. This result again assures us that Lasso is not as suitable as the credible sets approach when p is large.

Regarding the computation time, when pis small, the output of Lasso regression was generated simultaneously as I run the code, while the credible sets methods took less than 5 seconds. When I increase the dimension to 413, the frequentist Lasso took a few second, and the credible sets method took less than 20 seconds. In general, the proposed approach took longer time.

![WeChat65f90c3610079b261f801c58daaabe4e](https://github.com/user-attachments/assets/f46cbb10-cca5-455e-bb74-5e3988282f3c)

Next, in order to find out the exact variables selected, I examined the order path of each method. For convenience, I only selected the independent cases under low dimension and high dimension to study. A table including the first 13 variables added is shown above. Among these variables, “lstat”, “rm”, “ptratio”, “dis”, “nox”, “black”, “tax” frequently appear at the beginning. These variables correspond to % lower status of the population, average number of rooms per dwelling, pupil-teacher ratio by town, weighted distances to five Boston employment centers, nitric oxides concentration (parts per 10 million), 1000(Bk - 0.63)2 where Bk is the proportion of blacks by town, and full-value property-tax rate per $10,000. All methods selected a few noise variables, with the marginal approach in a high dimensional setting selecting the most. Among those original variables, “age” was never selected, indicating that proportion of owner-occupied units built prior to 1940 is not a necessary element determining the median value of owner-occupied homes in $1000’s. Moreover, we do notice that the marginal order path in high dimension is slightly different from the remaining three.

## Conclusion
In conclusion, for this project, I extensively summarized the paper of my choice and demonstrated how credible regions approach could be a more efficient method than other frequentist methods and SSVS. Then, I extended the paper by including my own application on Boston housing dataset using the proposed method and frequentist Lasso. In general my own application correspond to the statements in the paper. The penalized credible regions approach works well in both low dimension and high dimension. With an increase in correlation among variables, the proposed approach yields an even more desirable MS-O and MS-N. On the contrary, the frequentist Lasso has a poor performance when the dimension increases. However, in my experiment, the performance of Lasso did not worsen much as I switch from independent variables to correlated variables. This point is worth further study. Finally, in this paper, I only compared the proposed method with the Lasso regression. For further studies, I could consider fitting the same dataset using MCMC and other common methods for a more comprehensive comparison.
